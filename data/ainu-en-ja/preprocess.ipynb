{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Ainu Folklore Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We mannually downloaded the html files from [A Glossed Audio Corpus of Ainu Folklore](http://ainucorpus.ninjal.ac.jp/corpus/en/).  \n",
    "Assume that source html files stored in `html` directory.\n",
    "```\n",
    "data\n",
    "└── ainu-en-ja\n",
    "    └── html\n",
    "        ├── K7708241UP.html\n",
    "        ├── K7708242UP.html\n",
    "        ├── K7803231UP.html\n",
    "        ├── K7803232UP.html\n",
    "        ├── K7803233KY.html\n",
    "        ├── K7803233UP.html\n",
    "        ├── K7807152KY.html\n",
    "        ├── K7908051UP.html\n",
    "        ├── K8010291UP.html\n",
    "        └── K8109193UP.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Document-level metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of documents\n",
    "# (doc_id, en_title, ja_title, checkdigits)\n",
    "docs = [\n",
    "    (\"K7803231UP\", \"The Young Lad Raised by the Cat God\", u\"猫の神様に育てられた少年\", 254),\n",
    "    (\"K7708241UP\", \"Pananpe Escapes from the Demon's Hands\", u\"パナンペ鬼の手から逃れる\", 126),\n",
    "    (\"K7708242UP\", \"The Girl Who Gave the Bad Red Dog Poison\", u\"悪い赤犬に毒を飲ませた少女\", 308),\n",
    "    (\"K7803232UP\", \"The Poor Man Who Dug Up the Village Chief Wife’s Grave\", \n",
    "     u\"村長の奥さんの墓を掘り返した貧乏人\", 174),\n",
    "    (\"K7803233UP\", \"The Grapevines which Warded Off the Topattumi-night Raiders\", \n",
    "     u\"ぶどうづるの輪がトパットゥミを退けてくれた話\", 419),\n",
    "    (\"K7803233KY\", \"The Woman Who Became kemkacikappo Bird\", u\"ケﾑカチカッポになった女\", 181),\n",
    "    (\"K7807152KY\", \"The Goddess of Fire Fought with the Demon God from the End of the Earth\", \n",
    "     u\"火の女神が地の果ての魔神と戦った\", 125),\n",
    "    (\"K7908051UP\", \"The Bridge of Mist\", u\"霞の架け橋\", 377),\n",
    "    (\"K8010291UP\", \"The Rich Man from Cenpak\", u\"チェンパｸのニｼパの話\", 535),\n",
    "    (\"K8109193UP\", \"Godly Elder Sister Gets Rid of Bad Bear Father\", \n",
    "     u\"巫力の強い姉が、悪い熊親父を退治した\", 169)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_sentence(soup, sent):\n",
    "    \n",
    "    kana = [td.text for td in sent.find_all(\"td\", [\"kana\"])]\n",
    "    ainu = [td.text for td in sent.find_all(\"td\", [\"ainu\"])]\n",
    "    morpheme = [td.text for td in sent.find_all(\"td\", [\"morpheme\"])]\n",
    "    gloss_en = [td.text for td in sent.find_all(\"td\", [\"gloss_en\"])]\n",
    "    gloss_jp = [td.text for td in sent.find_all(\"td\", [\"gloss_jp\"])]\n",
    "    \n",
    "    return zip(kana, ainu, morpheme, gloss_en, gloss_jp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_translation(soup, sent):\n",
    "    \n",
    "    ja = [div.text for div in sent.find_all(\"div\", [\"ft_j\", \"translation-text\"])]\n",
    "    en = [div.text for div in sent.find_all(\"div\", [\"ft_e\", \"translation-text\"])]\n",
    "    return zip(en, ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from codecs import open\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def extract(doc, sentences, translations):\n",
    "    \n",
    "    # read file\n",
    "    with open(os.path.join(\"html\", \"%s.txt\" % doc), 'r', encoding=\"utf-8\") as f:\n",
    "        read_data = f.read()\n",
    "        \n",
    "    # parse html\n",
    "    soup = bs(read_data, 'html.parser')\n",
    "    \n",
    "    # extract entries\n",
    "    sents = soup.find_all(id=lambda x: x and x.startswith(\"gridview-1029-record-\"))\n",
    "    for sent in sents:\n",
    "        a = sent.find('span')\n",
    "        record_id = int(a['id'].split('.')[-1])\n",
    "        if not sentences.has_key((doc, record_id)):\n",
    "            sentences[(doc, record_id)] = find_sentence(soup, sent)\n",
    "        if not translations.has_key((doc, record_id)):\n",
    "            translations[(doc, record_id)] = find_translation(soup, sent)\n",
    "    \n",
    "    return sentences, translations, record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing K7803231UP ... 254 sequences found.\n",
      "processing K7708241UP ... 126 sequences found.\n",
      "processing K7708242UP ... 308 sequences found.\n",
      "processing K7803232UP ... 174 sequences found.\n",
      "processing K7803233UP ... 419 sequences found.\n",
      "processing K7803233KY ... 181 sequences found.\n",
      "processing K7807152KY ... 125 sequences found.\n",
      "processing K7908051UP ... 377 sequences found.\n",
      "processing K8010291UP ... 535 sequences found.\n",
      "processing K8109193UP ... 169 sequences found.\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "sentences = OrderedDict()\n",
    "translations = OrderedDict()\n",
    "for d in docs:\n",
    "    print 'processing', d[0], '...',\n",
    "    sentences, translations, record_id = extract(d[0], sentences, translations)\n",
    "    assert d[-1] == record_id\n",
    "    print record_id, 'sequences found.'\n",
    "print 'done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "from codecs import open\n",
    "\n",
    "with open('sentences.cPickle', 'wb') as f:\n",
    "    cPickle.dump(sentences, f)\n",
    "    \n",
    "with open('translations.cPickle', 'wb') as f:\n",
    "    cPickle.dump(translations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('sentences.cPickle', 'rb') as f:\n",
    "    sents = cPickle.load(f)\n",
    "    \n",
    "with open('translations.cPickle', 'rb') as f:\n",
    "    trans = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sanitize(ainu):\n",
    "    ainu = ainu.strip()\n",
    "    ainu = re.sub(r'\\(.+\\)', '', ainu)\n",
    "    ainu = re.sub(r'\\[.+\\]', '', ainu)\n",
    "    ainu = re.sub(r'[|_\\[\\]()]', '', ainu)\n",
    "    ainu = re.sub(r'<y>', '', ainu) # phonological alternations, insertions\n",
    "    ainu = re.sub(r'={2,}', '=', ainu)\n",
    "    ainu = re.sub(u'\\u201c', '\"', ainu) # double quotation\n",
    "    ainu = re.sub(u'\\u201d', '\"', ainu) # double quotation\n",
    "    ainu = re.sub(ur\"^[`'’]|[`'’]$\", '\"', ainu) # single quote\n",
    "    ainu = re.sub(ur\"['’]s \", \"'s \", ainu) # apostrophy\n",
    "    ainu = re.sub(ur\"['’]m \", \"'m \", ainu) # apostrophy\n",
    "    ainu = re.sub(ur\"['’]ve \", \"'ve \", ainu) # apostrophy\n",
    "    ainu = re.sub(ur\"['’]ll \", \"'ll \", ainu) # apostrophy\n",
    "    ainu = re.sub(ur\"n['’]t \", \"n't \", ainu) # apostrophy\n",
    "    ainu = re.sub(r\"\\*\\d{1,2}\", \"\", ainu) # footnote mark\n",
    "    ainu = re.sub(u'…', '...', ainu)\n",
    "    #ainu = re.sub(r'[.,;?\\']$', '', ainu)\n",
    "    return ainu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [neologdn](https://github.com/ikegami-yukino/neologdn) to normalize Japanese texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neologdn\n",
    "def sanitize_ja(ja):\n",
    "    ja = ja.strip()\n",
    "    ja = re.sub(u\"（？）\", \"(?)\", ja)\n",
    "    ja = re.sub(u\"（\", \"(\", ja)\n",
    "    ja = re.sub(u\"）\", \")\", ja)\n",
    "    ja = re.sub(ur\"\\[\", \"(\", ja)\n",
    "    ja = re.sub(ur\"\\]\", \")\", ja)\n",
    "    ja = re.sub(ur\"\\(\", \" (\", ja)\n",
    "    ja = re.sub(ur\"\\)\", \") \", ja)\n",
    "    ja = re.sub(u\"．\", \".\", ja)\n",
    "    ja = re.sub(u\"--\", \"...\", ja)\n",
    "    ja = re.sub(u\"…\", \"...\", ja)\n",
    "    ja = re.sub(ur\"^[`'’]|[`'’]$\", '\"', ja)\n",
    "    ja = re.sub(u\"『\", u\"「\", ja)\n",
    "    ja = re.sub(u\"』\", u\"」\", ja)\n",
    "    ja = re.sub(r\"\\*\", \"\", ja)\n",
    "    ja = neologdn.normalize(ja)\n",
    "    return ja.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(kana):\n",
    "    kana = re.sub(r\"\\?\", \" ? \", kana)\n",
    "    kana = re.sub(r\",{1}\", \" , \", kana)\n",
    "    kana = re.sub(r'\"', ' &quot; ', kana)\n",
    "    kana = re.sub(r\"\\.{1}\", \" . \", kana)\n",
    "    kana = re.sub(r\"\\s{2,}\", \" \", kana)\n",
    "    kana = re.sub(r'(\\.\\s\\.\\s\\.)', '...', kana)\n",
    "    kana = re.sub(r'(\\.\\s\\.)', '...', kana)\n",
    "    return kana.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save original texts and translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from codecs import open\n",
    "with open('train.tok.ainu', 'w', encoding='utf-8') as ainu_train:\n",
    "    with open('train.tok.kana', 'w', encoding='utf-8') as kana_train:\n",
    "        with open('dev.tok.ainu', 'w', encoding='utf-8') as ainu_dev:\n",
    "            with open('dev.tok.kana', 'w', encoding='utf-8') as kana_dev:\n",
    "                for k, v in sentences.iteritems():\n",
    "                    kana = ' '.join([tokenize(sanitize(t[0])) for t in v])\n",
    "                    ainu = ' '.join([tokenize(sanitize(t[1])) for t in v])\n",
    "                    if k[0] in [\"K8010291UP\", \"K8109193UP\"]:\n",
    "                        kana_dev.write(kana + '\\n')\n",
    "                        ainu_dev.write(ainu + '\\n')\n",
    "                    else:\n",
    "                        kana_train.write(kana + '\\n')\n",
    "                        ainu_train.write(ainu + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('train.en', 'w', encoding='utf-8') as en_train:\n",
    "    with open('train.ja', 'w', encoding='utf-8') as ja_train:\n",
    "        with open('dev.en', 'w', encoding='utf-8') as en_dev:\n",
    "            with open('dev.ja', 'w', encoding='utf-8') as ja_dev:\n",
    "                for k, v in translations.iteritems():\n",
    "                    if k[0] in [\"K8010291UP\", \"K8109193UP\"]:\n",
    "                        en_dev.write(sanitize(v[1][0]) + '\\n')\n",
    "                        ja_dev.write(sanitize_ja(v[0][0]) + '\\n')\n",
    "                    else:\n",
    "                        en_train.write(sanitize(v[1][0]) + '\\n')\n",
    "                        ja_train.write(sanitize_ja(v[0][0]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To tokenize, we use external scripts:  \n",
    "**eng:** `tokenizer.pl` provided as a component of moses [here](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl)  \n",
    "**jpn:** [Mecab](http://taku910.github.io/mecab/)'s `-Owkati` option (see `scripts/ja_tokenizer.py` for details)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "script_dir = os.path.join(os.getcwd(), '..', '..', 'scripts')\n",
    "ja_tokenizer = os.path.join(script_dir, 'ja_tokenizer.py')\n",
    "en_tokenizer = os.path.join(script_dir,  'tokenizer.pl')\n",
    "\n",
    "for d in ['train', 'dev']:\n",
    "    os.system('%s -l en <%s.en >%s.tok.en' % (en_tokenizer, d, d))\n",
    "    os.system('%s <%s.ja >%s.tok.ja' % (ja_tokenizer, d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Format them in xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_meta(soup, doc_id, en_title, ja_title):\n",
    "    metadata = soup.new_tag(\"metadata\")\n",
    "    \n",
    "    if doc_id.endswith(\"UP\"):\n",
    "        genre = \"Uepeker\"\n",
    "    elif doc_id.endswith(\"KY\"):\n",
    "        genre = \"Kamuyyukar\"\n",
    "    \n",
    "    # ainu\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'language'\n",
    "    meta['name'] = 'ainu'\n",
    "    meta['scripts'] = 'latin,kana'\n",
    "    meta['iso-639-3'] = 'ain'\n",
    "    meta['tiers'] = 'phrases words morphemes'\n",
    "    metadata.append(meta)\n",
    "    \n",
    "\n",
    "    # eng\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'language'\n",
    "    meta['name'] = 'english'\n",
    "    meta['iso-639-3'] = 'eng'\n",
    "    meta['tiers'] = 'glosses translations'\n",
    "    metadata.append(meta)\n",
    "\n",
    "    # jpn\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'language'\n",
    "    meta['name'] = 'japanese'\n",
    "    meta['iso-639-3'] = 'jpn'\n",
    "    meta['tiers'] = 'glosses translations'\n",
    "    metadata.append(meta)\n",
    "\n",
    "    # doc_id\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'source'\n",
    "    meta['id'] = doc_id\n",
    "    meta['genre'] = genre\n",
    "    metadata.append(meta)\n",
    "    \n",
    "    # title\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'title'\n",
    "    meta['language'] = 'eng'\n",
    "    meta.append(en_title)\n",
    "    metadata.append(meta)\n",
    "    \n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['type'] = 'title'\n",
    "    meta['language'] = 'jpn'\n",
    "    meta.append(ja_title)\n",
    "    metadata.append(meta)\n",
    "\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent(soup, k, v):\n",
    "    igt = soup.new_tag(\"igt\")\n",
    "    igt['id'] = k[0] + '.%0.3d' % k[1]\n",
    "    \n",
    "    # raw sentence\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'phrases'\n",
    "    tier['id'] = 'r.%0.3d' % k[1]\n",
    "    tier['state'] = 'raw'\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'r_ainu.%0.3d' % k[1]\n",
    "    item['script'] = 'latin'\n",
    "    item.append(' '.join([t[1] for t in v]))\n",
    "    tier.append(item)\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'r_kana.%0.3d' % k[1]\n",
    "    item['script'] = 'kana'\n",
    "    item.append(' '.join([t[0] for t in v]))\n",
    "    tier.append(item)\n",
    "    igt.append(tier)\n",
    "    \n",
    "    # cleaned sentence\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'phrases'\n",
    "    tier['id'] = 'c.%0.3d' % k[1]\n",
    "    tier['state'] = 'cleaned'\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'c_ainu.%0.3d' % k[1]\n",
    "    item['script'] = 'latin'\n",
    "    item.append(' '.join([sanitize(t[1]) for t in v]))\n",
    "    tier.append(item)\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'c_kana.%0.3d' % k[1]\n",
    "    item['script'] = 'kana'\n",
    "    item.append(' '.join([sanitize(t[0]) for t in v]))\n",
    "    tier.append(item)\n",
    "    igt.append(tier)\n",
    "    \n",
    "    # tokenized sentence\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'phrases'\n",
    "    tier['id'] = 'n.%0.3d' % k[1]\n",
    "    tier['state'] = 'normalized'\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'n_ainu.%0.3d' % k[1]\n",
    "    item['script'] = 'latin'\n",
    "    item.append(' '.join([tokenize(sanitize(t[1])) for t in v]))\n",
    "    tier.append(item)\n",
    "    igt.append(tier)\n",
    "    \n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['id'] = 'n_kana.%0.3d' % k[1]\n",
    "    item['script'] = 'kana'\n",
    "    item.append(' '.join([tokenize(sanitize(t[0])) for t in v]))\n",
    "    tier.append(item)\n",
    "    igt.append(tier)\n",
    "    return igt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def notes(soup, orig, align):\n",
    "    meta = soup.new_tag(\"meta\")\n",
    "    meta['alignment'] = ','.join(align)\n",
    "    tmp = orig[0]\n",
    "    if len(orig) > 1:\n",
    "        tmp = orig[1]\n",
    "    \n",
    "    if re.match(r'.+\\*\\d{1,2}$', tmp):\n",
    "        meta['type'] = 'footnote'\n",
    "        meta.append('')\n",
    "    elif re.search(r'\\([^\\?]+\\)', tmp):\n",
    "        meta['type'] = 'phonological alternations' # ()\n",
    "        match = re.search(r'\\(([^\\?]+)\\)', tmp)\n",
    "        meta.append(match.group(1))\n",
    "    else:\n",
    "        if re.search(r'_', tmp):\n",
    "            meta['type'] = 'phonological alternations' # underbar\n",
    "        elif re.search(r'\\(\\?\\)', tmp):\n",
    "            meta['type'] = 'unclear interpretations' # (?)\n",
    "        elif re.search(r'<.+>', tmp):\n",
    "            meta['type'] = 'inserted sounds' # <>\n",
    "        elif re.search(r'\\[.+\\]', tmp):\n",
    "            meta['type'] = 'complementary' # []\n",
    "        meta.append(' '.join(orig))\n",
    "    return meta\n",
    "        \n",
    "\n",
    "def ainu_words(soup, k, v):\n",
    "    tier_id = 'ainu.%0.3d' % k[1]\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'words'\n",
    "    tier['id'] = 'w_' + tier_id\n",
    "    tier['script'] = 'latin'\n",
    "    tier['segmentation'] = 'n_' + tier_id\n",
    "    \n",
    "    metadata = soup.new_tag(\"metadata\")\n",
    "    flag = False\n",
    "    \n",
    "    c = 0\n",
    "    for i, w in enumerate(v):\n",
    "        clean = tokenize(sanitize(w[1]))\n",
    "        tok = clean.split()\n",
    "        for j, t in enumerate(tok):\n",
    "            align = 'w_' + tier_id + '.%d' % (i+1)\n",
    "            if len(tok) > 1:\n",
    "                align += '.%d' % (j+1)\n",
    "            if len(v) > i and t != v[i][1]:\n",
    "                if not re.match(r'^\\&quot;.+|.+(\\.|,|\\?|\\&quot;)$', clean):\n",
    "                    meta = notes(soup, [v[i][1]], [align])\n",
    "                    metadata.append(meta)\n",
    "                    flag = True\n",
    "            item = soup.new_tag(\"item\")\n",
    "            item['id'] = align\n",
    "            item['segmentation'] = 'n_' + tier_id + \"[%d:%d]\" % (c, c+len(t))\n",
    "            item.append(t)\n",
    "            tier.append(item)\n",
    "            c += 1 + len(t)\n",
    "    if flag:\n",
    "        tier.insert(0, metadata)\n",
    "    return tier\n",
    "\n",
    "\n",
    "    \n",
    "def kana_words(soup, k, v):\n",
    "    tier_id = 'kana.%0.3d' % k[1]\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'words'\n",
    "    tier['id'] = 'w_' + tier_id\n",
    "    tier['script'] = 'kana'\n",
    "    tier['segmentation'] = 'n_' + tier_id\n",
    "    \n",
    "    metadata = soup.new_tag(\"metadata\")\n",
    "    flag = False\n",
    "    \n",
    "    c = 0\n",
    "    for i, w in enumerate(v):\n",
    "        clean = tokenize(sanitize(w[0]))\n",
    "        tok = clean.split()\n",
    "        \n",
    "        for j, t in enumerate(tok):\n",
    "            align = 'w_' + tier_id + '.%d' % (i+1)\n",
    "            if len(tok) > 1:\n",
    "                align += '.%d' % (j+1)\n",
    "            if len(v) > i and t != v[i][0]:\n",
    "                if not re.match(r'^\\&quot;.+|.+(\\.|,|\\?|\\&quot;)$', clean):\n",
    "                    orig_array = [v[i][0]]\n",
    "                    align_array = [align]\n",
    "                    if i > 0:\n",
    "                        orig_array.insert(0, v[i-1][0])\n",
    "                        align_array.insert(0, 'w_' + tier_id + '.%d' % (i))\n",
    "                    meta = notes(soup, orig_array, align_array)\n",
    "                    metadata.append(meta)\n",
    "                    flag = True\n",
    "            item = soup.new_tag(\"item\")\n",
    "            item['id'] = align\n",
    "            item['segmentation'] = 'n_' + tier_id + \"[%d:%d]\" % (c, c+len(t))\n",
    "            item.append(t)\n",
    "            tier.append(item)\n",
    "            c += 1 + len(t)\n",
    "    if flag:\n",
    "        tier.insert(0, metadata)\n",
    "    return tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def morpheme(soup, k, v):\n",
    "    tier_id = '.%0.3d' % k[1]\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'morphemes'\n",
    "    tier['id'] =  'm' + tier_id\n",
    "    tier['segmentation'] = 'w_ainu' + tier_id\n",
    "    \n",
    "    metadata = soup.new_tag(\"metadata\")\n",
    "    flag = False\n",
    "    \n",
    "    for i, w in enumerate(v):\n",
    "        word = w[2]\n",
    "        morph = word.split('-')\n",
    "        \n",
    "        # check consistency -> gloss\n",
    "        #if w[2].count('-') != w[3].count('-'):\n",
    "        #    print k, '\\t', w[2], '\\t', w[3]\n",
    "            \n",
    "            \n",
    "        c = 0\n",
    "        for j, t in enumerate(morph):\n",
    "            align = 'm' + tier_id + '.%d' % (i+1)\n",
    "            if len(morph) > 1:\n",
    "                align += '.%d' % (j+1)\n",
    "            if len(v) > i and t != v[i][2]:\n",
    "                if not (re.match(r'.+(\\.|,|\\?)$', word) or re.search(r'-', word)):\n",
    "                    meta = notes(soup, [v[i][2]], [align])\n",
    "                    metadata.append(meta)\n",
    "                    flag = True\n",
    "            item = soup.new_tag(\"item\")\n",
    "            item['id'] = align\n",
    "            item['segmentation'] = 'w_ainu' + tier_id + \".%d[%d:%d]\" % (i+1, c, c+len(t))\n",
    "            item.append(t)\n",
    "            tier.append(item)\n",
    "            c += len(t)\n",
    "    if flag:\n",
    "        tier.insert(0, metadata)\n",
    "    return tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gloss(soup, k, v, lang):\n",
    "    tier_id = '.%0.3d' % k[1]\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'glosses'\n",
    "    tier['id'] =  lang + '_g' + tier_id\n",
    "    tier['language'] = lang\n",
    "    tier['alignment'] = 'm' + tier_id\n",
    "    \n",
    "    metadata = soup.new_tag(\"metadata\")\n",
    "    flag = False\n",
    "    \n",
    "    if lang == 'eng':\n",
    "        idx = 3\n",
    "    elif lang == 'jpn':\n",
    "        idx = 4\n",
    "    else:\n",
    "        raise Exception('language definition not found.')\n",
    "    \n",
    "    \n",
    "    for i, w in enumerate(v):\n",
    "        morph = re.sub(ur'(\\u2010|\\uff0d|\\u30fc)', '-', w[2])\n",
    "        gloss = w[idx]\n",
    "        if gloss in [u'\\u2010', u'\\uff0d', u'\\u30fc']:\n",
    "            gloss = re.sub(ur'(\\u2010|\\uff0d|\\u30fc)', u'\\u2015', gloss)\n",
    "        if re.search(ur'in\\u2010law', gloss):\n",
    "            gloss = re.sub(ur'(\\uff0d|\\u30fc)', '-', gloss)\n",
    "        else:\n",
    "            gloss = re.sub(ur'(\\u2010|\\uff0d|\\u30fc)', '-', gloss)\n",
    "        \n",
    "        \n",
    "        \n",
    "        glosses = gloss.split('-')\n",
    "        morphs = morph.split('-')\n",
    "        \n",
    "        # check consistency\n",
    "        if len(morphs) != len(glosses):\n",
    "            print '!', k, i, '\\t', morph,'\\t', len(morphs), '\\t', gloss, '\\t', len(glosses)\n",
    "            #raise ValueError('glosses not consistent with mophemes')\n",
    "            continue\n",
    "            \n",
    "        c = 0\n",
    "        for j, (m, g) in enumerate(zip(morphs, glosses)):\n",
    "            align = lang + '_g' + tier_id + '.%d' % (i+1)\n",
    "            if len(glosses) > 1:\n",
    "                align += '.%d' % (j+1)\n",
    "            if len(v) > i and m != v[i][idx]:\n",
    "                if not (re.match(r'.+(\\.|,|\\?)$', m) or re.search(r'-', m)):\n",
    "                    meta = notes(soup, v[i][idx], align)\n",
    "                    metadata.append(meta)\n",
    "                    flag = True\n",
    "            \n",
    "            gloss_elm = g.split('.')\n",
    "            en = []\n",
    "            pos = []\n",
    "            for b, gl in enumerate(gloss_elm):\n",
    "                if lang == 'jpn':\n",
    "                    en_g = w[3]\n",
    "                    if en_g in [u'\\u2010', u'\\uff0d', u'\\u30fc']:\n",
    "                        en_g = re.sub(ur'(\\u2010|\\uff0d|\\u30fc)', u'\\u2015', en_g)\n",
    "                    if re.search(ur'in\\u2010law', en_g):\n",
    "                        en_g = re.sub(ur'(\\uff0d|\\u30fc)', '-', en_g)\n",
    "                    else:\n",
    "                        en_g = re.sub(ur'(\\u2010|\\uff0d|\\u30fc)', '-', en_g)\n",
    "                    try:\n",
    "                        en_gloss = en_g.split('-')[j].split('.')[b]\n",
    "                    except IndexError:\n",
    "                        print '$', k, i, '\\t', en_g,'\\t', len(en_g.split('-')),\\\n",
    "                                    '\\t', gloss, '\\t', len(glosses)\n",
    "                        #raise ValueError('jpn glosses not consistent with eng glosses')\n",
    "                        continue\n",
    "                else:\n",
    "                    en_gloss = gl\n",
    "                if en_gloss.isupper() or (en_gloss.isdigit() and int(en_gloss) <= 4):\n",
    "                    pos.append(gl)\n",
    "                else:\n",
    "                    en.append(gl)\n",
    "            \n",
    "            if len(en) > 0:\n",
    "                item = soup.new_tag(\"item\")\n",
    "                item['id'] = align + '_' + lang\n",
    "                item['alignment'] = 'm_ainu' + align[5:]\n",
    "                item.append(' '.join(en))\n",
    "                tier.append(item)\n",
    "            \n",
    "            if len(pos) > 0:\n",
    "                item = soup.new_tag(\"item\")\n",
    "                item['id'] = align + '_tag'\n",
    "                item['alignment'] = 'm_ainu' + align[5:]\n",
    "                item.append('.'.join(pos))\n",
    "                tier.append(item)\n",
    "            \n",
    "            c += len(m)\n",
    "        \n",
    "\n",
    "    return tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translation(soup, k, t):\n",
    "    tier_id = '.%0.3d' % k[1]\n",
    "    tier = soup.new_tag(\"tier\")\n",
    "    tier['type'] = 'translations'\n",
    "    tier['id'] =  't' + tier_id\n",
    "    tier['alignment'] = 'r' + tier_id\n",
    "    \n",
    "    # english\n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['language'] = 'eng'\n",
    "    item.append(sanitize(t[1][0]))\n",
    "    tier.append(item)\n",
    "    \n",
    "    # japanese\n",
    "    item = soup.new_tag(\"item\")\n",
    "    item['language'] = 'jpn'\n",
    "    item.append(sanitize_ja(t[0][0]))\n",
    "    tier.append(item)\n",
    "    return tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct(doc_id, sentences, translations, en_title, ja_title):\n",
    "    \n",
    "    # corpus\n",
    "    soup = bs(\"\", \"xml\")\n",
    "    corpus = soup.new_tag(\"xigt-corpus\")\n",
    "    \n",
    "    # doc meta\n",
    "    meta = doc_meta(soup, doc_id, en_title, ja_title)\n",
    "    corpus.append(meta)\n",
    "    \n",
    "    i = 1\n",
    "    k = (doc_id, i)\n",
    "    \n",
    "    #for k, v in sentences.iteritems():\n",
    "    while sentences.has_key(k):\n",
    "        v = sentences[k]\n",
    "        # sentence meta\n",
    "        igt = sent(soup, k, v)\n",
    "\n",
    "        # words\n",
    "        ainu = ainu_words(soup, k, v)\n",
    "        igt.append(ainu)\n",
    "        kana = kana_words(soup, k, v)\n",
    "        igt.append(kana)\n",
    "\n",
    "        # morphemes\n",
    "        morph = morpheme(soup, k, v)\n",
    "        igt.append(morph)\n",
    "\n",
    "        # glosses\n",
    "        try:\n",
    "            en_gloss = gloss(soup, k, v, 'eng')\n",
    "            igt.append(en_gloss)\n",
    "            ja_gloss = gloss(soup, k, v, 'jpn')\n",
    "            igt.append(ja_gloss)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # translations\n",
    "        t = translations[k]\n",
    "        trans = translation(soup, k, t)\n",
    "        igt.append(trans)\n",
    "\n",
    "        corpus.append(igt)\n",
    "        i += 1\n",
    "        k = (doc_id, i)\n",
    "    \n",
    "    \n",
    "    soup.append(corpus)\n",
    "    return soup, i-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detected following incontistencies, even after sanitaization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! ('K7803231UP', 168) 7 \to-yan \t2 \tat.APPL-land-INTR.SG \t3\n",
      "! ('K7803231UP', 168) 7 \to-yan \t2 \t～に-陸-自形成.単 \t3\n",
      "! ('K7803231UP', 252) 7 \tko-hosipi \t2 \t～に帰る \t1\n",
      "! ('K7708242UP', 110) 2 \tkatun \t1 \tshape-belong.to \t2\n",
      "! ('K7708242UP', 110) 2 \tkatun \t1 \t姿/形-～に属している \t2\n",
      "! ('K7708242UP', 111) 2 \tkatun \t1 \tshape-belong.to \t2\n",
      "! ('K7708242UP', 111) 2 \tkatun \t1 \t姿/形-～に属している \t2\n",
      "! ('K7708242UP', 151) 2 \tkatun \t1 \tshape-belong.to \t2\n",
      "! ('K7708242UP', 151) 2 \tkatun \t1 \t姿/形-～に属している \t2\n",
      "! ('K7708242UP', 152) 2 \tkatun \t1 \tshape-belong.to \t2\n",
      "! ('K7708242UP', 152) 2 \tkatun \t1 \t姿/形-～に属している \t2\n",
      "! ('K7708242UP', 165) 6 \te-u-ka-opiwki \t4 \t～について-互い―上-助ける? \t3\n",
      "! ('K7708242UP', 170) 4 \t- \t2 \t― \t1\n",
      "! ('K7708242UP', 170) 4 \t- \t2 \t― \t1\n",
      "! ('K7708242UP', 172) 10 \t- \t2 \t― \t1\n",
      "! ('K7708242UP', 172) 10 \t- \t2 \t― \t1\n",
      "! ('K7708242UP', 231) 5 \tkatun \t1 \tshape-belong.to \t2\n",
      "! ('K7708242UP', 231) 5 \tkatun \t1 \t姿/形-～に属している \t2\n",
      "! ('K7708242UP', 287) 3 \t- \t2 \t― \t1\n",
      "! ('K7708242UP', 287) 3 \t- \t2 \t― \t1\n",
      "! ('K7803232UP', 32) 5 \t- \t2 \t― \t1\n",
      "! ('K7803232UP', 32) 5 \t- \t2 \t― \t1\n",
      "! ('K7803232UP', 37) 8 \tyay-ko-an \t3 \tREFL-with.APPL.exist.SG \t2\n",
      "$ ('K7803232UP', 37) 8 \tREFL-with.APPL.exist.SG \t2 \t自分-～と共に-ある.単 \t3\n",
      "$ ('K7803232UP', 37) 8 \tREFL-with.APPL.exist.SG \t2 \t自分-～と共に-ある.単 \t3\n",
      "! ('K7803232UP', 96) 9 \tku-w-ari \t3 \t弓-挿入～を置く.複 \t2\n",
      "! ('K7803233UP', 52) 2 \t- \t2 \t― \t1\n",
      "! ('K7803233UP', 52) 2 \t- \t2 \t― \t1\n",
      "! ('K7803233UP', 73) 9 \t- \t2 \t― \t1\n",
      "! ('K7803233UP', 73) 9 \t- \t2 \t― \t1\n",
      "! ('K7803233UP', 74) 3 \t- \t2 \t*** \t1\n",
      "! ('K7803233UP', 74) 3 \t- \t2 \t*** \t1\n",
      "! ('K7803233UP', 76) 0 \to-si-so-un \t4 \t尻.～の.接頭-右座-～にはまる \t3\n",
      "! ('K7803233UP', 105) 5 \tkosmat-i \t2 \tdaughter-in-law-POSS \t4\n",
      "! ('K7803233UP', 219) 3 \tpayokay \t1 \tgo.PL-exist.PL \t2\n",
      "! ('K7803233UP', 219) 3 \tpayokay \t1 \t行く.複-ある.複 \t2\n",
      "! ('K7803233UP', 264) 4 \tpayoka \t1 \tgo.PL-exist.PL \t2\n",
      "! ('K7803233UP', 264) 4 \tpayoka \t1 \t行く.複-ある.複 \t2\n",
      "! ('K7803233UP', 277) 1 \tpayoka \t1 \tgo.PL-exist.PL \t2\n",
      "! ('K7803233UP', 277) 1 \tpayoka \t1 \t行く.複-ある.複 \t2\n",
      "$ ('K7803233UP', 299) 6 \tand \t1 \tながら|ある.複 \t1\n",
      "$ ('K7803233UP', 299) 10 \tCOP \t1 \t完了.単 \t1\n",
      "$ ('K7803233UP', 299) 12 \tthing/person \t1 \t4.目 \t1\n",
      "! ('K7803233UP', 342) 2 \tokkay-po-utar-i \t4 \t男性-指小-複～の \t3\n",
      "! ('K7803233KY', 44) 2 \tya-o-raypa \t3 \tland-to.APPL-move-TR.PL \t4\n",
      "! ('K7803233KY', 44) 2 \tya-o-raypa \t3 \t陸-～に-(～を)動(かす)-他形成.複 \t4\n",
      "! ('K7803233KY', 159) 1 \ttasiro-ho \t2 \tsword \t1\n",
      "! ('K7803233KY', 159) 1 \ttasiro-ho \t2 \t山刀 \t1\n",
      "$ ('K7807152KY', 18) 1 \tstrength-superior.INTR \t2 \t力-多い.自形成.単 \t2\n",
      "! ('K7807152KY', 125) 10 \t- \t2 \t― \t1\n",
      "! ('K7807152KY', 125) 10 \t- \t2 \t― \t1\n",
      "! ('K7908051UP', 141) 5 \tko-i-ani \t3 \tto.APPL-APASS-hold-TR.SG \t4\n",
      "! ('K7908051UP', 141) 5 \tko-i-ani \t3 \t～に-もの-(～を)持(つ)-他形成.単 \t4\n",
      "! ('K7908051UP', 153) 2 \tko-an-i \t3 \tto.APPL-hold-hold-TR.SG \t4\n",
      "$ ('K7908051UP', 153) 2 \tto.APPL-hold-hold-TR.SG \t4 \t～に-(～を)持(つ)-他形成.単 \t3\n",
      "! ('K7908051UP', 189) 6 \t- \t2 \t― \t1\n",
      "! ('K7908051UP', 189) 6 \t- \t2 \t― \t1\n",
      "! ('K7908051UP', 293) 5 \tmaka \t1 \topen-TR.PL \t2\n",
      "! ('K7908051UP', 293) 5 \tmaka \t1 \t(～を)開(ける)-他形成.単 \t2\n",
      "! ('K7908051UP', 316) 12 \tko-tom-ka? \t3 \tto.APPL-be.beautiful.CAUS? \t2\n",
      "$ ('K7908051UP', 316) 12 \tto.APPL-be.beautiful.CAUS? \t2 \t～に対して-美しい-～させる? \t3\n",
      "! ('K7908051UP', 327) 1 \tkokow \t1 \tson-in-law \t3\n",
      "! ('K8010291UP', 119) 0 \to-harki-so-un \t4 \tbottom.PF.POSS-left.(from.the.east.window)-seat.near.fireplace-fit.in|fire-end-space? \t6\n",
      "! ('K8010291UP', 119) 1 \tape-kes-utur? \t3 \ttoward \t1\n",
      "! ('K8010291UP', 119) 3 \tpeka \t1 \tquiet-ADV \t2\n",
      "! ('K8010291UP', 119) 4 \tmo-no \t2 \tsit.SG \t1\n",
      "$ ('K8010291UP', 119) 1 \ttoward \t1 \t火-端-間? \t3\n",
      "$ ('K8010291UP', 119) 1 \ttoward \t1 \t火-端-間? \t3\n",
      "$ ('K8010291UP', 119) 4 \tsit.SG \t1 \t静かだ-副 \t2\n",
      "$ ('K8010291UP', 159) 13 \tat.APPL-leave-PL \t3 \t～に-(～を)残(す)-他形成.複 \t3\n",
      "! ('K8010291UP', 208) 2 \tarsike-no \t2 \tbe.empty-handed-ADV? \t3\n",
      "! ('K8010291UP', 339) 1 \t- \t2 \t― \t1\n",
      "! ('K8010291UP', 339) 2 \t- \t2 \t― \t1\n",
      "! ('K8010291UP', 339) 1 \t- \t2 \t― \t1\n",
      "! ('K8010291UP', 339) 2 \t- \t2 \t― \t1\n",
      "! ('K8010291UP', 465) 2 \titak \t1 \t話- \t2\n",
      "! ('K8010291UP', 513) 1 \tsiwto \t1 \tone's.in-laws.POSS \t2\n",
      "! ('K8010291UP', 514) 7 \tan-i \t2 \tby/with \t1\n",
      "! ('K8010291UP', 514) 7 \tan-i \t2 \t～によって \t1\n",
      "! ('K8109193UP', 32) 6 \to-si-so-un \t4 \t尻.～の.接頭-右座-～にはまる \t3\n",
      "! ('K8109193UP', 59) 4 \tonisippakor \t1 \tat.APPL-root-have \t3\n",
      "! ('K8109193UP', 59) 4 \tonisippakor \t1 \t～に-根-～を持つ \t3\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    soup, _ = construct(d[0], sentences, translations, d[1], d[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mannually corrected the inconcistencies above, and then saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing K7803231UP ... 254 sequences saved.\n",
      "processing K7708241UP ... 126 sequences saved.\n",
      "processing K7708242UP ... 308 sequences saved.\n",
      "processing K7803232UP ... 174 sequences saved.\n",
      "processing K7803233UP ... 419 sequences saved.\n",
      "processing K7803233KY ... 181 sequences saved.\n",
      "processing K7807152KY ... 125 sequences saved.\n",
      "processing K7908051UP ... 377 sequences saved.\n",
      "processing K8010291UP ... 535 sequences saved.\n",
      "processing K8109193UP ... 169 sequences saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from codecs import open\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "for d in docs:\n",
    "    print 'processing', d[0], '...',\n",
    "    with open(os.path.join('xml', '%s.xml' % d[0]), 'w', encoding='utf-8') as f:\n",
    "        soup, i = construct(d[0], sents, trans, d[1], d[2])\n",
    "        assert d[-1] == i\n",
    "        print i, 'sequences saved.'\n",
    "        f.write(re.sub('&amp;', '&', soup.prettify()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Call from xigt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xigt.codecs import xigtxml\n",
    "with open(os.path.join('xml', 'K7803231UP.xml')) as f:\n",
    "    xc = xigtxml.load(f)\n",
    "    words_tier = xc[0]['w_ainu.001']\n",
    "    morph_tier = xc[0]['m.001']\n",
    "    gloss_tier = xc[0]['eng_g.001']\n",
    "    jpn_tier = xc[0]['jpn_g.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne ene iki wa okay pe a= ne ru an hi ka a= erampewtek no ,\n",
      "             ne            what              なに\n",
      "            ene       like this           このように\n",
      "              i           APASS              もの\n",
      "             ki              do            ～をする\n",
      "             wa             and              して\n",
      "           okay           exist              ある\n",
      "             pe              PL               複\n",
      "             a=    thing/person              もの\n",
      "             ne            4.A=           4.他主=\n",
      "             ru             COP            ～である\n",
      "             an         INFR.EV              こと\n",
      "             hi           exist              ある\n",
      "             ka              SG               単\n",
      "             a=            NMLZ               の\n",
      "     erampewtek            even               も\n",
      "             no            4.A=           4.他主=\n"
     ]
    }
   ],
   "source": [
    "print ' '.join([w.value().strip() for w in words_tier])\n",
    "for m, g, j in zip(morph_tier, gloss_tier, jpn_tier):\n",
    "    print '%15s %15s %15s' % (m.value().strip(), g.value().strip(), j.value().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            sent token vocab\n",
      "----------------------------\n",
      "K7803231UP   254  2249   537\n",
      "K7708241UP   126   874   234\n",
      "K7708242UP   308  2828   560\n",
      "K7803232UP   174  1556   391\n",
      "K7803233UP   419  3704   695\n",
      "K7803233KY   181   803   239\n",
      "K7807152KY   125   477   173\n",
      "K7908051UP   377  3531   608\n",
      "K8010291UP   535  4945   829\n",
      "K8109193UP   169  1827   468\n",
      "----------------------------\n",
      "     total  2668 22794  2141\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "x = []\n",
    "y = 0\n",
    "print '%10s %5s %5s %5s' % ('', 'sent', 'token', 'vocab')\n",
    "print '-'*28\n",
    "for d in docs:\n",
    "    i = 1\n",
    "    vocab = []\n",
    "    while sents.has_key((d[0], i)):\n",
    "        v = sents[(d[0], i)]\n",
    "        vocab.extend([w[1] for w in v])\n",
    "        i += 1\n",
    "    x.extend(vocab)\n",
    "    y += d[-1]\n",
    "    print '%10s %5d %5d %5d' % (d[0], d[-1], len(vocab), len(Counter(vocab)))\n",
    "print '-'*28\n",
    "print '%10s %5d %5d %5d' % ('total', y, len(x), len(Counter(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of tokens occur just once (or more than 10 times) in the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1053"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i for i in Counter(x).values() if i==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in Counter(x).values() if i > 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‐\n"
     ]
    }
   ],
   "source": [
    "print u'\\u2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
